---
title: "CLUSTER JERARQUICO"
author: "Daniel BarandiarÃ¡n"
date: '2022-04-14'
output: html_document
---

El anÃ¡lisis cluster jerÃ¡rquico (hierarchical clustering, HC) permite agrupar las observaciones en funciÃ³n de su similitud y obtener la estructura de asociaciÃ³n en cadena de las observaciones (llamado dendrograma).Esto permite elegir cualquier nÃºmero de grupos cortando el Ã¡rbol al nivel correcto.

En este curso utilizaremos la agrupaciÃ³n aglomerativa (Agglomerative Nesting o AGNES) parte de tantos grupos como individuos y se van agrupando hasta llegar a tener a todos los individuos en un solo grupo. TambiÃ©n llamada aproximaciÃ³n â€œbottom-upâ€ o ascendente. 

AquÃ­ hay algunos detalles a tener en cuenta:
- Genera una jerarquÃ­a de agrupaciÃ³n (dendrograma).
- Si hay muchas observaciones puede ser difÃ­cil de interpretar.
- Puede consumir mucho tiempo. Alternativas: BIRCH,CURE, y CHAMALEON.
- No requiere especificar nÃºmero de grupos.
- Es mÃ¡s determinÃ­stico.
- No requiere iteraciÃ³n.
- Existen 2 algoritmos: aglomerativo o divisivo.
- Utiliza una matriz de distancias de cualquier tipo.
- Utiliza un Ãºnico mÃ©todo de vinculaciÃ³n.


MÃ©todos de vinculaciÃ³n entre grupos:

- MÃ©todo de distancia completa "Complete": tambiÃ©n llamado de distancia mÃ¡xima o de VECINO MÃS LEJANO. Utiliza la distancia mÃ¡xima (del cluster) y une dos grupos para formar uno con el menor diÃ¡metro. Crea grupos compactos, no captura comportamientos locales y es sensible a valores atÃ­picos.
- MÃ©todo de distancia Ãºnica "Single": tambiÃ©n llamado de distancia mÃ­nima o VECINO MÃS CERCANO. Captura comportamientos locales, pero ignora la estructura completa, con lo cual es sensible a valores atÃ­picos y al ruido.
- MÃ©todo de distancia promedio "UPGMA": utiliza la distancia promedio de los puntos de un cluster.
- MÃ©todo de distancia al centroide "Centroide": utiliza la distancia al centroide, pueden ser difÃ­ciles de interpretar.
- MÃ©todo de Ward: es un mÃ©todo de agrupaciÃ³n de varianza mÃ­nima que crea grupos compactos y de tamaÃ±o uniforme.

El mÃ©todo WARD y el COMPLETO son preferible cuando se quiere que los grupos sean homogeneos, y aproximadamente del mismo tamaÃ±o.
El mÃ©todo SINGLE encuentra de Ã¡rbol de mÃ­nima expansiÃ³n (el Ã¡rbol mÃ¡s corto que conecta los puntos), pero puede generar grupos con tamaÃ±os desigual.

EXTRA: Â¿QuÃ© mÃ©todo de vinculaciÃ³n es mejor?
Si deseas evaluar cuÃ¡l de los mÃ©todos se adapta mejor a tus datos, puedes cacular la correlaciÃ³n cofenÃ©tica o la distancia de Gower.

- CorrelaciÃ³n cofenÃ©tica: 
EvaluÌa queÌ tan bien el cluster preserva las distancias originales entre cada par de observaciones.
Mide la correlacioÌn entre las distancias originales y las distancias estimadas a partir del aÌrbol de agrupacioÌn.
[-1, 1], 0=representacioÌn no adecuada.
Mayor correlacioÌn -> Mejor representacioÌn.
Ejemplo en R: Utiliza la funcioÌn cophenetic()
```{r}
# datos originales
x <- data.frame(X=c(1,0,3),Y=c(2,0,0), row.names=c("A","B","C"))
# agrupacioÌn con el meÌtodo de vinculacioÌn elegido
x_hc <- hclust(dist(x), method="complete")
# distancias cofeneÌticas
cof1 <- cophenetic(x_hc)
# correlacioÌn entre las distancias originales y las distancias cofeneÌtica 
cor(dist(x),cof1)
```


- Distancia de Gower
Misma idea de fondo: comparar las distancias originales con las distancias del aÌrbol para ver queÌ tan bien se adecuÌa el aÌrbol a la relacioÌn original.
Pero le da maÌs importancia a los casos donde las medidas de distancia original y cofeneÌtica son diferentes.
Calcula la suma cuadraÌtica de las diferencias entre las distancias de los datos originales y las distancias cofeneÌticas.
[0, 1], 0=representacioÌn adecuada.
Menor distancia -> Mejor representacioÌn.
Ejemplo en R: compara las distancias originales con las distancias cofenÃ©ticas

```{r}
sum((dist(x)-cof1)^2)
```

Ejemplo.
Para crear el cluster:

hclust(ğ‘‘, ğ‘šğ‘’ğ‘¡hğ‘œğ‘‘ = "ğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¡ğ‘’")

```{r}
mtcars_std <- scale(mtcars[c("hp", "mpg")])
mtcars_dist <- dist(mtcars_std)
mtcars_hc <- hclust(mtcars_dist, method="complete") #mÃ©todo completo (vecinos lejanos)
```

Otras opciones para especificar el mÃ©todo de agrupaciÃ³n:

 "ğ‘¤ğ‘ğ‘Ÿğ‘‘.ğ·2" para el meÌtodo de varianza miÌnima,
 "ğ‘ ğ‘–ğ‘›ğ‘”ğ‘™ğ‘’" para el meÌtodo de distancia miÌnima,
 "ğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¡ğ‘’" para el meÌtodo de distancia maÌxima,
 "ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’" para el meÌtodo de distancia promedio (= UPGMA), 
 "ğ‘ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘–ğ‘‘" para el meÌtodo de distancia al centroide.
Existe alguna otra opcioÌn pero de menor importancia.

Para graficar el resultado (dendrograma) utiliza la funciÃ³n plot().
```{r}
par(cex=0.5, mar=c(5,8,4,1)) #para modificar el tamaÃ±o y los mÃ¡rgenes
plot(mtcars_hc, axes = FALSE) #axes para quitar o poner los ejes
```


Â¿CÃ³mo elegir el nÃºmero final de grupos? 

Para construir los grupos debemos "cortar" el Ã¡rbol (dendrograma) segÃºn altura. O podemos elegir el nÃºmero de grupos de forma arbitraria.

Para cortar en k=2
```{r}
library(factoextra)
fviz_dend(mtcars_hc, k=2, cex=0.5, rect=TRUE) #rect para marcar con cuadros a puntos
```

Para cortar en k=3
```{r}
fviz_dend(mtcars_hc, k=3, cex=0.5, rect=TRUE) #rect para marcar con cuadros a puntos
```

Podemos obtener la agrupaciÃ³n de forma no grÃ¡fica.
```{r}
grupos <- cutree(mtcars_hc, k=3)
grupos
```
Nos muestra a que grupo pertenece cada coche.

Podemos aÃ±adir la etiqueta/variable de a que grupo pertenece dentro de los datos.
```{r}
mtcars$grupos <- as.factor(grupos)
head(mtcars[c("hp", "mpg", "grupos")])
```

Graficar la agrupaciÃ³n junto con las variables.

```{r}
library(ggrepel)
ggplot(mtcars[c("hp", "mpg", "grupos")], aes(hp, mpg, color=grupos, label=rownames(mtcars))) + geom_text_repel(size=3) #repel para que no se solapne las etiquetas
```

Se pueden explorar las caracterÃ­sitcas de cada cluster, por ejemplo la cantidad de componenetes o la media de cada cluster en funcion de cada variable.
```{r}
table(mtcars$grupos)
```
```{r}
by(mtcars[c("mpg", "hp")], mtcars$grupos, colMeans)
```

REPRESENTACION E INTERPRETACIÃ“N

Queremos realizar agrupaciones de coches segÃºn los caballos, el consumo y el peso.
```{r}
mtcars_std <- as.data.frame(scale(mtcars[,c("hp", "mpg", "wt")]))
mtcars_hc <- hclust(dist(mtcars_std), method = "ward.D2")
grupos <- cutree(mtcars_hc, k=4)
table(grupos)
```
Vemos en un grÃ¡fico.
```{r}
library(factoextra)
fviz_dend(mtcars_hc, k=4, cex = .2, horiz = T)
```

Vemos que los grupos azul y morado son los mÃ¡s similares mientras que el rojo es el mÃ¡s distinto.

Podemos ver tmabiÃ©n un dendrograma circular, mÃ¡s usado en ambito genÃ©tico.
```{r}
fviz_dend(mtcars_hc, k=4, cex = .3, type="circular", color_labels_by_k = T)
```

Endrograma estilo filogenia
```{r}
library(igraph)
fviz_dend(mtcars_hc, k=4, cex = .3, type="phylogenic", color_labels_by_k = T, repel=T)
```


Resumen por grupo. para obtener la media o la desviaciÃ³n tÃ­pica de cada variable por grupos.
```{r}
aggregate(mtcars[,c("hp", "mpg", "wt")], list(grupos), mean)
```
```{r}
aggregate(mtcars[,c("hp", "mpg", "wt")], list(grupos), sd)
```

En nuestro caso queremos valores sin escalar, para interpretarlos mejor.
Vemos los resultados con las diferencias entre cada grupo y variables.

Para combinar un dendrograma con los valores de las variables podemos usar un mapa de calor. Se pude hacer de varias maneras pero con la funciÃ³n pheatmap de la librerÃ­a con el mismo nombre nos permite personalizarlo y es muy Ãºtil.
```{r}
library(pheatmap)
pheatmap(mtcars_std, cluster_cols = FALSE, scale = "none", cutree_rows = 4, fontsize = 8, clustering_distance_cols = "euclidean", clustering_method = "ward.D2")
```

En el grÃ¡fico vemos los grupos separados y en quÃ© variables son las que mÃ¡s destacan o mÃ¡s similares son entre sÃ­.

Otra forma de representar los cluster es mediante un grÃ¡fico de dispersiÃ³n. De este modo podemos graficar de forma bidimensional los cluster en funciÃ³n de dos variables.
```{r}
fviz_cluster(list(data=mtcars_std, cluster=grupos), choose.vars = c("hp", "wt"), labelsize = 8, repel = TRUE)
```

Para obtener el grÃ¡fico de dispersiÃ³n para las tres variables que estÃ¡bamos estudiando podemos utilizar el grÃ¡fico de anÃ¡lisis de componentes principales (PCA) mÃ¡s cluster.
```{r}
library(factoextra)
fviz_cluster(list(data=mtcars_std, cluster=grupos), labelsize = 8, repel = TRUE)
```

Las tres variables se combinan en dos nuevas dimensiones. Vemos que la dimensiÃ³n 1 es la mÃ¡s importante.

Por Ãºltimo podemos realizar un grÃ¡fico radar, o circulares. Nos permiten identicar los grupos mÃ¡s equilibrados en relaciÃ³n a las variables de estudio.
Aqui las variables se escalan en un rango de 0 a 100 para visualizarlos de forma conjunta.
```{r}
df <- as.data.frame(mtcars_std)+2 #cambio de escala
df <- aggregate(df, list(grupos), mean) #media
colnames(df)[1] <-"group"
#devtools::install_github("xl0418/ggradar2",dependencies=TRUE)
library(ggradar2)
ggradar2(df)
```

Vemos que el 1 es el mÃ¡s equilibrado, vemos que los vÃ©rtices de este grupo se encuentran bÃ¡sicamente al 50%, mientra que los grupos 3 y 4 destacan en alguna de las variables.

Recuerda que los resultados del anÃ¡lisis cluster depende de las decisiones tomadas previamente, como la preparaciÃ³n de los datos, la medida de distancia, el mÃ©todo de agrupaciÃ³n utilizado, y el nÃºmero de grupos seleccionados. Revisando todos estos con los resultados esperados y usando tÃ©cinas de validaciÃ³n de agrupaciÃ³n para obtener los mejores resultados










