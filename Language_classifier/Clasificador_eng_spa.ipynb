{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdown import markdown\n",
    "\n",
    "with open('README.md', 'r') as file:\n",
    "    contenido_md = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Team API â€“ ACOPS\\n\\nA Team API is a description and specification that ACOPS team has defined that tells others how to interact with that team.\\n\\n## Team API\\n\\nDate: 01-06-2024\\n\\n* Team name and focus: AC-OPS (Automation Community - Operations).\\n* Team type: Operations.\\n* Part of a Platform? (y/n) Details: The ACOPS team will help the Strategic and Technical teams to more successfully achieve their roadmaps and objectives.\\n* Do we provide a service to other teams? Yes Details: Providing technical support, incident resolution, and ensuring system functionality.\\n* What kind of Service Level Expectations do other teams have of us? Collaboration to achieve their objetives.\\n* Software owned and evolved by this team:\\n* Versioning approaches:\\n* Wiki search terms: AC-OPS, Automation Operations, Automation Community Operations.\\n* Chat tool channels: #ac-ops-internal #Microsoft Teams chat-.\\n* Time of daily sync meeting: Daily Misa for 30 min, from 09:00 to 09:30 AM. Daily meeting for 15 min from 10:55 to 11:10. (Monday to Friday).\\n* Administrative silence rule: When this team requests information from any other team, we reserve the right to close the request after 48 hours without any response.\\n\\n### What we're currently working on\\n\\n* Our services and systems: Providing support, resolving incidents, reviewing failures, and creating automated solutions.\\n* Ways of working: Collaborating closely with other teams, driving automation transformation.\\n* Wider cross-team or organisational improvements: Stability, continuous progress, automation advancement.\\n\\n### Teams we currently interact with\\n\\n| Team name/focus | Interaction Mode |         Purpose         | Duration |\\n| --------------- | ---------------- | ----------------------- | -------- |\\n| . IAAS          | Collaboration    | Infrastructure support  | Ongoing  |\\n| . HERMES        | Collaboration    | Communication support   | Ongoing  |\\n| . ANSIBLE       | Collaboration    | Support                 | Ongoing  |\\n| . ACCROSS       | Collaboration    | Support                 | Ongoing  |\\n| . [LANDING ZONE](https://marketplace.mapfre.com/docs/default/mapfredocument/CloudSite/lz/)| Collaboration    | Support                 | Ongoing  |\\n\\n### Teams we expect to interact with soon\\n\\n| Team name/focus | Interaction Mode | Purpose | Duration |\\n| --------------- | ---------------- | ------- | -------- |\\n| --------------- | ---------------- | ------- | -------- |\\n| --------------- | ---------------- | ------- | -------- |\\n| --------------- | ---------------- | ------- | -------- |\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contenido_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag\n",
    "\n",
    "def extract_person_entities(tokens):\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    tree = ne_chunk(tagged_tokens)\n",
    "    \n",
    "    person_entities = []\n",
    "    for subtree in tree:\n",
    "        if isinstance(subtree, nltk.Tree) and subtree.label() == 'PERSON':\n",
    "            entity = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            person_entities.append(entity)\n",
    "\n",
    "    non_person_tokens = [token for token, pos in tagged_tokens if token not in person_entities]\n",
    "\n",
    "    return non_person_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Words: ['service', 'closely', 'system', 'interact', 'approach', 'administrative', 'resolution', 'tell', 'own', 'platform', 'technical', 'achieve', 'way', 'hour', 'close', 'create', 'https', 'automation', 'help', 'service', 'soon', 'friday', 'organisational', 'meet', 'work', 'transformation', 'provide', 'community', 'detail', 'time', 'specification', 'collaborate', 'provide', 'team', 'sync', 'ensure', 'rule', 'continuous', 'incident', 'progress', 'response', 'mode', 'functionality', 'channel', 'min', 'term', 'purpose', 'daily', 'interaction', 'description', 'stability', 'failure', 'infrastructure', 'define', 'resolve', 'kind', 'support', 'tool', 'name', 'silence', 'expectation', 'yes', 'part', 'currently', 'solution', 'type', 'incident', 'information', 'duration', 'date', 'focus', 'strategic', 'objective', 'expect', 'wide', 'drive', 'request', 'automate', 'system', 'zone', 'we', 'level', 'operation', 'request', 'evolve', 'roadmap', 'monday', 'communication', 'reserve', 'search', 'advancement', 'successfully', 'right', 'improvement', 'review', 'collaboration', 'without', 'team']\n",
      "Spanish Words: ['chat', 'other', 'versioning', 'misar', 'microsoft', 'objetiv']\n",
      "English: 94.23%\n",
      "Spanish: 5.77%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Cargar modelos de spaCy para inglés y español\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = contenido_md\n",
    "\n",
    "# Eliminar palabras que estén completamente en mayúsculas\n",
    "text = ' '.join(word for word in text.split() if not word.isupper())\n",
    "\n",
    "# Eliminar dígitos\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Tokenizar el texto\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# EXTRACCION ENTITIES \n",
    "tokens = extract_person_entities(tokens)\n",
    "\n",
    "# Lista de idiomas y sus códigos para stopwords\n",
    "stopwords_lang = {\n",
    "    \"spanish\": set(stopwords.words(\"spanish\")),\n",
    "    \"english\": set(stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "# Eliminar stopwords para cada idioma\n",
    "tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "tokens = [token for token in tokens if token not in stopwords_lang[\"spanish\"] and token not in stopwords_lang[\"english\"]]\n",
    "\n",
    "# Funciones de idioma usando WordNet\n",
    "def is_english_word(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    return len(synsets) > 0\n",
    "\n",
    "def is_spanish_word(word):\n",
    "    synsets = wordnet.synsets(word, lang='spa')\n",
    "    return bool(synsets)\n",
    "\n",
    "# Diccionario adicional de palabras comunes en inglés y español no reconocidas por WordNet\n",
    "additional_english_words = set(['other', 'roadmaps', 'objetiv', 'without','recomended', 'test'])\n",
    "additional_spanish_words = set([\"facilitator\", \"si\", \"alto\", \"principal\", \"actual\", \"continuo\", \"favor\", \"idea\", \"junto\", \"toda\"])\n",
    "\n",
    "# Calcular probabilidades de pertenencia a cada idioma y clasificar palabras\n",
    "def classify_words(tokens):\n",
    "    english_words = set()\n",
    "    spanish_words = set()\n",
    "\n",
    "    for token in tokens:\n",
    "        english = is_english_word(token) or token in additional_english_words\n",
    "        spanish = is_spanish_word(token) or token in additional_spanish_words\n",
    "\n",
    "        if english and not spanish:\n",
    "            english_words.add(token)\n",
    "        elif spanish and not english:\n",
    "            spanish_words.add(token)\n",
    "        else:\n",
    "            # Si la palabra es ambigua, analiza el contexto usando N-Gramas\n",
    "            context = get_context(tokens, token)\n",
    "            if context:\n",
    "                english_prob = calculate_ngram_probability(context, 'english')\n",
    "                spanish_prob = calculate_ngram_probability(context, 'spanish')\n",
    "                if english_prob > spanish_prob:\n",
    "                    english_words.add(token)\n",
    "                else:\n",
    "                    spanish_words.add(token)\n",
    "\n",
    "    return english_words, spanish_words\n",
    "\n",
    "def get_context(tokens, target, window_size=2):\n",
    "    index = tokens.index(target)\n",
    "    start = max(index - window_size, 0)\n",
    "    end = min(index + window_size + 1, len(tokens))\n",
    "    context = tokens[start:end]\n",
    "    return context\n",
    "\n",
    "def calculate_ngram_probability(context, language):\n",
    "    context_text = ' '.join(context)\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), vocabulary=stopwords_lang[language])\n",
    "    ngram_counts = vectorizer.fit_transform([context_text]).toarray()\n",
    "    return ngram_counts.sum()\n",
    "\n",
    "# Clasificar palabras\n",
    "english_words, spanish_words = classify_words(tokens)\n",
    "\n",
    "# Lematizar palabras clasificadas por idioma\n",
    "english_lemmatized = [nlp_en(word)[0].lemma_ for word in english_words]\n",
    "spanish_lemmatized = [nlp_es(word)[0].lemma_ for word in spanish_words]\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"English Words:\", english_lemmatized)\n",
    "print(\"Spanish Words:\", spanish_lemmatized)\n",
    "\n",
    "# Calcular y mostrar los porcentajes\n",
    "total_words = len(english_lemmatized) + len(spanish_lemmatized)\n",
    "english_percentage = (len(english_lemmatized) / total_words) * 100 if total_words > 0 else 0\n",
    "spanish_percentage = (len(spanish_lemmatized) / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "print(f\"English: {english_percentage:.2f}%\")\n",
    "print(f\"Spanish: {spanish_percentage:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
